{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "together-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joint-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#此處 M,D指係數矩陣\n",
    "def legendre(x,M):\n",
    "    return torch.matmul(M,torch.Tensor([[1],[x],[0.5*(3*(x**2)-1)],[0.5*(5*(x**3)-3*x)]]))\n",
    "def dlegendre(x,D):\n",
    "    return torch.matmul(D,torch.Tensor([[0],[1],[3*x],[0.5*(15*(x**2)-3)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "russian-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y=True)\n",
    "y = y[:,np.newaxis]\n",
    "\n",
    "X = Variable(torch.Tensor(X))\n",
    "y = Variable(torch.LongTensor(y))\n",
    "\n",
    "dataset = torch.cat((X,y),1)\n",
    "dataset = DataLoader(dataset,shuffle=True)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in dataset:\n",
    "    temp = i.numpy().squeeze()\n",
    "    X.append(temp[0:4])\n",
    "    y.append(int(temp[4]))\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train = Variable(torch.Tensor(X[:125,:]))\n",
    "y_train = Variable(torch.LongTensor(y[:125]))\n",
    "\n",
    "X_test = Variable(torch.Tensor(X[125:,:]))\n",
    "y_test = Variable(torch.LongTensor(y[125:]))\n",
    "#X共150筆 4個feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rough-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (1): ODE_block(\n",
      "    (h1): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (h2): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (h3): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (h4): Linear(in_features=4, out_features=4, bias=True)\n",
      "  )\n",
      "  (2): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (3): LogSoftmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ODE_block(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1=torch.nn.Linear(4,4)\n",
    "        self.h2=torch.nn.Linear(4,4)\n",
    "        self.h3=torch.nn.Linear(4,4)\n",
    "        self.h4=torch.nn.Linear(4,4)\n",
    "    def forward(self,x):\n",
    "        x1=F.relu(self.h1(x))\n",
    "        x2=F.relu(self.h2(x1))\n",
    "        x3=F.relu(self.h3(x2))\n",
    "        x4=F.relu(self.h4(x3))\n",
    "        self.x1=x1\n",
    "        self.x2=x2\n",
    "        self.x3=x3\n",
    "        self.x4=x4\n",
    "        return x4\n",
    "up_layer = [torch.nn.Linear(4,4)]\n",
    "middle_layer = [ODE_block()]\n",
    "down_layer = [torch.nn.Linear(4,3),torch.nn.LogSoftmax()]\n",
    "net = torch.nn.Sequential(*up_layer,*middle_layer,*down_layer)\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generous-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:119: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "#train ODE BLOCK\n",
    "t = torch.Tensor([-1*(sqrt(525+70*sqrt(30)))/(35),-1*(sqrt(525-70*sqrt(30)))/(35),(sqrt(525-70*sqrt(30)))/(35),(sqrt(525+70*sqrt(30)))/(35)])\n",
    "M = Variable(torch.Tensor(0.5*np.random.randn(4,4)),requires_grad=True)\n",
    "aopt = torch.optim.Adam(net[1].parameters(),lr=0.001)\n",
    "gama = 0.4\n",
    "lr = 0.001\n",
    "epoch = 100\n",
    "\n",
    "for itr in range(epoch):\n",
    "    pick = random.randrange(0,len(X_train))\n",
    "    x = X_train[pick]\n",
    "    pd = net(x)\n",
    "    loss1 = gama*loss_func(pd.unsqueeze(0),y_train[pick].unsqueeze(0))#gama為超參數\n",
    "    loss2 = 0\n",
    "    tp = [net[1].x1,net[1].x2,net[1].x3,net[1].x4]\n",
    "    for j in range(len(t)):\n",
    "        if j == 0 or j == 3:\n",
    "            loss2 += ((18-sqrt(30))/36)*torch.norm(dlegendre(t[j],M)-tp[j])\n",
    "        else:\n",
    "            loss2 += ((18+sqrt(30))/36)*torch.norm(dlegendre(t[j],M)-tp[j])\n",
    "    total_loss = loss1+loss2\n",
    "    if total_loss < 1e-5:\n",
    "        print(\"Finished...\")\n",
    "        break\n",
    "    total_loss.backward(retain_graph = True)\n",
    "    M.data.zero_()\n",
    "    M.data.sub_(lr*M.grad.data)\n",
    "    \n",
    "    aopt.zero_grad()\n",
    "    loss2.backward()\n",
    "    aopt.step()\n",
    "    \n",
    "print(\"Training completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "artificial-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5234, grad_fn=<AddBackward0>)\n",
      "tensor([0.3937, 0.8338, 0.1498, 0.3305], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0570, 0.0000, 0.0000, 0.3730], grad_fn=<ReluBackward0>)\n",
      "tensor([0.0000, 0.0000, 0.0679, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([0., 0., 0., 0.], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(total_loss,net[1].x1,net[1].x2,net[1].x3,net[1].x4,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "correct-lobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "#train up and down layer\n",
    "aopt1 = torch.optim.Adam(net[0].parameters(),lr=0.001)\n",
    "aopt2 = torch.optim.Adam(net[2].parameters(),lr=0.001)\n",
    "for i in range(500):\n",
    "    pd = net(X_train)\n",
    "    loss = loss_func(pd,y_train)\n",
    "    aopt1.zero_grad()\n",
    "    aopt2.zero_grad()\n",
    "    loss.backward()\n",
    "    aopt1.step()\n",
    "    aopt2.step()\n",
    "    \n",
    "print(\"Training completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "professional-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predict(x):\n",
    "    l = []\n",
    "    temp = net(x)\n",
    "    for i in temp:\n",
    "        i=list(i)\n",
    "        l.append(i.index(max(i)))\n",
    "    return l\n",
    "\n",
    "def cal_accuracy(yp,yt):\n",
    "    ans = list(np.array(yp)-np.array(yt))\n",
    "    print(\"accuracy:%.2f\"%(ans.count(0)/len(yt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "covered-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "accuracy:0.32\n"
     ]
    }
   ],
   "source": [
    "l=class_predict(X_test)\n",
    "print(l)\n",
    "cal_accuracy(l,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-consumption",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
