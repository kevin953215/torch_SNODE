{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "together-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joint-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#此處 M,D指係數矩陣\n",
    "def legendre(x,M):\n",
    "    return torch.matmul(M,torch.Tensor([[1],[x],[0.5*(3*(x**2)-1)],[0.5*(5*(x**3)-3*x)]]))\n",
    "def dlegendre(x,D):\n",
    "    return torch.matmul(D,torch.Tensor([[0],[1],[3*x],[0.5*(15*(x**2)-3)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "russian-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.X,self.y = load_iris(return_X_y=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return torch.Tensor(self.X[index]),torch.LongTensor([self.y[index]])\n",
    "\n",
    "bs = 10 #batch_size\n",
    "all_data = my_dataset()\n",
    "my_dataloader = DataLoader(all_data, batch_size = bs, num_workers = 0, drop_last = True, shuffle = True)\n",
    "\n",
    "all_xdata = []\n",
    "all_ydata = []\n",
    "\n",
    "for x,y in my_dataloader:\n",
    "    all_xdata.append(x)\n",
    "    all_ydata.append(y)\n",
    "    \n",
    "X_test =  all_xdata[0:2]\n",
    "y_test =  all_ydata[0:2]\n",
    "\n",
    "X_train = all_xdata[2:]\n",
    "y_train = all_ydata[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rough-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (1): ODE_block(\n",
      "    (h1): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (h2): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (h3): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (h4): Linear(in_features=4, out_features=4, bias=True)\n",
      "  )\n",
      "  (2): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (3): LogSoftmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ODE_block(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1=torch.nn.Linear(4,4)\n",
    "        self.h2=torch.nn.Linear(4,4)\n",
    "        self.h3=torch.nn.Linear(4,4)\n",
    "        self.h4=torch.nn.Linear(4,4)\n",
    "    def forward(self,x):\n",
    "        x1=F.relu(self.h1(x))\n",
    "        x2=F.relu(self.h2(x1))\n",
    "        x3=F.relu(self.h3(x2))\n",
    "        x4=F.relu(self.h4(x3))\n",
    "        self.x1=x1\n",
    "        self.x2=x2\n",
    "        self.x3=x3\n",
    "        self.x4=x4\n",
    "        return x4\n",
    "up_layer = [torch.nn.Linear(4,4)]\n",
    "middle_layer = [ODE_block()]\n",
    "down_layer = [torch.nn.Linear(4,3),torch.nn.LogSoftmax()]\n",
    "net = torch.nn.Sequential(*up_layer,*middle_layer,*down_layer)\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generous-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:119: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "#train ODE BLOCK\n",
    "t = torch.Tensor([-1*(sqrt(525+70*sqrt(30)))/(35),-1*(sqrt(525-70*sqrt(30)))/(35),(sqrt(525-70*sqrt(30)))/(35),(sqrt(525+70*sqrt(30)))/(35)])\n",
    "M = Variable(torch.Tensor(0.5*np.random.randn(bs,4,4)),requires_grad=True)\n",
    "aopt = torch.optim.Adam(net[1].parameters(),lr=0.001)\n",
    "aopt1 = torch.optim.Adam(net[0].parameters(),lr=0.001)\n",
    "aopt2 = torch.optim.Adam(net[2].parameters(),lr=0.001)\n",
    "gama = 0.5\n",
    "lr = 0.001\n",
    "epoch = 600\n",
    "\n",
    "for itr in range(epoch):\n",
    "    if itr % 10 < 5:\n",
    "        pick = random.randrange(0,len(X_train))\n",
    "        x = X_train[pick]\n",
    "        pd = net(x)\n",
    "        loss1 = gama*loss_func(pd,y_train[pick].squeeze())#gama為超參數\n",
    "        loss2 = 0\n",
    "        tp = [net[1].x1,net[1].x2,net[1].x3,net[1].x4]\n",
    "        for j in range(len(t)):\n",
    "            temp = dlegendre(t[j],M).squeeze()\n",
    "            if j == 0 or j == 3:\n",
    "                loss2 += ((18-sqrt(30))/36)*torch.mean(torch.norm(tp[j]-temp,dim=1))\n",
    "            else:\n",
    "                loss2 += ((18+sqrt(30))/36)*torch.mean(torch.norm(tp[j]-temp,dim=1))\n",
    "        total_loss = loss1+loss2\n",
    "        if total_loss < 1e-5:\n",
    "            print(\"Finished...\")\n",
    "            break\n",
    "        total_loss.backward(retain_graph = True)\n",
    "        M.data.zero_()\n",
    "        M.data.sub_(lr*M.grad.data)\n",
    "    \n",
    "        aopt.zero_grad()\n",
    "        loss2.backward()\n",
    "        aopt.step()\n",
    "    else:\n",
    "        pick = random.randrange(0,len(X_train))\n",
    "        x = X_train[pick]\n",
    "        pd = net(x)\n",
    "        loss = loss_func(pd,y_train[pick].squeeze())\n",
    "        aopt1.zero_grad()\n",
    "        aopt2.zero_grad()\n",
    "        loss.backward()\n",
    "        aopt1.step()\n",
    "        aopt2.step()\n",
    "        \n",
    "print(\"Training completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "professional-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predict(x):\n",
    "    l = []\n",
    "    for i in x:\n",
    "        temp = net(i)\n",
    "        for i in temp:\n",
    "            i = list(i)\n",
    "            l.append(i.index(max(i)))\n",
    "    return l\n",
    "\n",
    "def cal_accuracy(yp,yt):\n",
    "    l = np.array([])\n",
    "    for i in yt:\n",
    "        l = np.append(l,np.array(i).squeeze())\n",
    "    ans = list(np.array(yp)-l)\n",
    "    print(\"accuracy:%.2f\"%(ans.count(0)/len(ans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "covered-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "accuracy:0.30\n"
     ]
    }
   ],
   "source": [
    "l=class_predict(X_test)\n",
    "print(l)\n",
    "cal_accuracy(l,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-survival",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
